<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OpenAI Realtime WebSocket Audio Test</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 1000px;
      margin: 0 auto;
      padding: 20px;
    }
    button {
      padding: 10px 20px;
      font-size: 16px;
      margin: 5px;
      background: #007bff;
      color: white;
      border: none;
      border-radius: 5px;
      cursor: pointer;
    }
    button:hover { background: #0056b3; }
    button:disabled {
      background: #6c757d;
      cursor: not-allowed;
    }
    .record-btn { background: #dc3545; }
    .record-btn:hover { background: #c82333; }
    .download-btn { background: #6f42c1; }
    .download-btn:hover { background: #5a2d91; }
    .recording {
      background: #28a745 !important;
      animation: pulse 1s infinite;
    }
    @keyframes pulse {
      0% { opacity: 1; }
      50% { opacity: 0.7; }
      100% { opacity: 1; }
    }
    #log {
      background: #f8f9fa;
      border: 1px solid #dee2e6;
      border-radius: 5px;
      padding: 10px;
      height: 300px;
      overflow-y: auto;
      font-family: monospace;
      white-space: pre-wrap;
    }
    audio {
      width: 100%;
      margin: 10px 0;
    }
    .controls {
      margin-bottom: 20px;
    }
    .transcript {
      background: #e3f2fd;
      padding: 10px;
      margin: 10px 0;
      border-left: 4px solid #2196f3;
      font-weight: bold;
    }
    .ai-text {
      background: #f3e5f5;
      padding: 10px;
      margin: 10px 0;
      border-left: 4px solid #9c27b0;
      font-style: italic;
    }
    .transcription-result {
      background: #fff3cd;
      padding: 10px;
      margin: 10px 0;
      border-left: 4px solid #ffc107;
      font-weight: bold;
    }
    .info-box {
      background: #d4edda;
      padding: 15px;
      margin: 10px 0;
      border-left: 4px solid #28a745;
      border-radius: 5px;
    }
  </style>
</head>
<body>
  <h1>Raw PCM16 Audio Capture - No MediaRecorder</h1>
  
  <div class="info-box">
    <strong>üéØ This captures raw PCM16 audio directly, bypassing MediaRecorder compression!</strong><br>
    Should provide better transcription quality by avoiding compression artifacts.
  </div>
  
  <div class="controls">
    <button id="connectBtn">Connect to Realtime API</button>
    <button id="recordBtn" class="record-btn" disabled>üé§ Start Raw Recording</button>
    <button id="downloadBtn" class="download-btn" disabled>‚¨áÔ∏è Download Raw PCM</button>
    <button id="transcribeBtn" class="download-btn" disabled>üìù Transcribe Raw Audio</button>
  </div>
  
  <pre id="log"></pre>
  <audio id="player" controls></audio>

  <script>
    let ws;
    let audioChunks = [];
    let responseAudioChunks = [];
    let isConnected = false;
    let isRecording = false;
    let aiTextBuffer = '';
    
    // Raw audio capture variables
    let audioContext;
    let sourceNode;
    let processorNode;
    let micStream;
    let rawPCMChunks = [];
    let lastRawBase64 = '';

    const API_KEY = "";

    function log(msg) {
      const logEl = document.getElementById("log");
      const timestamp = new Date().toLocaleTimeString();
      logEl.textContent += `[${timestamp}] ${msg}\n`;
      logEl.scrollTop = logEl.scrollHeight;
    }

    // Convert Float32Array to PCM16 ArrayBuffer
    function floatTo16BitPCM(float32Array) {
      const buffer = new ArrayBuffer(float32Array.length * 2);
      const view = new DataView(buffer);
      let offset = 0;
      for (let i = 0; i < float32Array.length; i++, offset += 2) {
        let s = Math.max(-1, Math.min(1, float32Array[i]));
        view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
      }
      return buffer;
    }

    // Convert raw PCM bytes to base64
    function pcmToBase64(pcmBytes) {
      let binary = '';
      const chunkSize = 0x8000; // 32KB chunks
      
      for (let i = 0; i < pcmBytes.length; i += chunkSize) {
        let chunk = pcmBytes.subarray(i, i + chunkSize);
        binary += String.fromCharCode.apply(null, chunk);
      }
      return btoa(binary);
    }

    // Start raw PCM16 recording
    async function startRawRecording() {
      try {
        log("üé§ Starting raw PCM16 capture...");
        
        // Get microphone stream
        micStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            sampleRate: 24000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
          }
        });

        // Create audio context at 24kHz (matching OpenAI requirements)
        audioContext = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 24000
        });

        log(`üîß AudioContext created: ${audioContext.sampleRate}Hz`);

        // Create source from microphone
        sourceNode = audioContext.createMediaStreamSource(micStream);
        
        // Create processor to capture raw samples (4096 samples per callback)
        processorNode = audioContext.createScriptProcessor(4096, 1, 1);
        
        processorNode.addEventListener('audioprocess', (e) => {
          if (isRecording) {
            const inputBuffer = e.inputBuffer;
            const channelData = inputBuffer.getChannelData(0); // Float32Array
            
            // Convert directly to PCM16 bytes
            const pcm16Buffer = floatTo16BitPCM(channelData);
            const pcmBytes = new Uint8Array(pcm16Buffer);
            
            // Store raw PCM bytes
            rawPCMChunks.push(pcmBytes);
            
            log(`üìä Captured ${channelData.length} samples (${pcmBytes.length} bytes PCM16)`);
          }
        });

        // Connect: Microphone ‚Üí Processor ‚Üí Destination (for monitoring)
        sourceNode.connect(processorNode);
        processorNode.connect(audioContext.destination);
        
        isRecording = true;
        rawPCMChunks = [];
        
        log("‚úÖ Raw PCM16 recording active - no compression!");
        return true;

      } catch (error) {
        log(`‚ùå Raw recording error: ${error.message}`);
        return false;
      }
    }

    // Stop raw recording and prepare data
    function stopRawRecording() {
      isRecording = false;
      
      if (processorNode) {
        processorNode.disconnect();
        sourceNode.disconnect();
        audioContext.close();
      }
      
      if (micStream) {
        micStream.getTracks().forEach(track => track.stop());
      }
      
      // Combine all PCM16 chunks
      const totalLength = rawPCMChunks.reduce((sum, chunk) => sum + chunk.length, 0);
      log(`üîß Combining ${rawPCMChunks.length} chunks, total ${totalLength} bytes`);
      
      const combinedPCM = new Uint8Array(totalLength);
      let offset = 0;
      
      for (const chunk of rawPCMChunks) {
        combinedPCM.set(chunk, offset);
        offset += chunk.length;
      }
      
      // Convert to base64 (no decode step needed!)
      lastRawBase64 = pcmToBase64(combinedPCM);
      
      log(`‚úÖ Raw PCM16 ready: ${lastRawBase64.length} chars base64, ${totalLength} bytes PCM`);
      log(`üìä Duration: ${(totalLength / 2 / 24000).toFixed(2)} seconds at 24kHz`);
      
      // Enable buttons
      document.getElementById("downloadBtn").disabled = false;
      document.getElementById("transcribeBtn").disabled = false;
      
      return lastRawBase64;
    }

    // Toggle recording
    async function toggleRecording() {
      const recordBtn = document.getElementById("recordBtn");
      
      if (!isRecording) {
        const success = await startRawRecording();
        if (success) {
          recordBtn.textContent = "üõë Stop Recording";
          recordBtn.classList.add("recording");
        }
      } else {
        const base64Audio = stopRawRecording();
        recordBtn.textContent = "üé§ Start Raw Recording";
        recordBtn.classList.remove("recording");
        
        // Send to Realtime API if connected
        if (isConnected && base64Audio) {
          sendToRealtimeAPI(base64Audio);
        }
      }
    }

    // Send raw PCM to Realtime API
    function sendToRealtimeAPI(base64Audio) {
      log("üì§ Sending raw PCM16 to Realtime API...");
      responseAudioChunks = []; // Clear previous response
      
      ws.send(JSON.stringify({
        type: 'input_audio_buffer.append',
        audio: base64Audio
      }));
      
      ws.send(JSON.stringify({type: 'input_audio_buffer.commit'}));
      ws.send(JSON.stringify({type: 'response.create'}));
    }

    // Download raw PCM as WAV file
    function downloadRawPCM() {
      if (!lastRawBase64) {
        log("‚ùå No raw PCM data to download");
        return;
      }

      try {
        // Convert base64 back to binary
        const binaryString = atob(lastRawBase64);
        const bytes = new Uint8Array(binaryString.length);
        
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        
        // Create WAV file from raw PCM16
        const wavData = createWavFile(bytes, 24000, 1);
        const blob = new Blob([wavData], { type: 'audio/wav' });
        
        // Download
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `raw_pcm16_${Date.now()}.wav`;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        URL.revokeObjectURL(url);
        
        log(`‚úÖ Downloaded: ${a.download}`);
        
      } catch (error) {
        log(`‚ùå Download error: ${error.message}`);
      }
    }

    // Create WAV file from PCM16 data
    function createWavFile(audioData, sampleRate, numChannels) {
      const length = audioData.length;
      const buffer = new ArrayBuffer(44 + length * 2);
      const view = new DataView(buffer);

      const writeString = (offset, string) => {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      };

      // WAV header
      writeString(0, 'RIFF');
      view.setUint32(4, 36 + length * 2, true);
      writeString(8, 'WAVE');
      writeString(12, 'fmt ');
      view.setUint32(16, 16, true);
      view.setUint16(20, 1, true);
      view.setUint16(22, numChannels, true);
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, sampleRate * 2, true);
      view.setUint16(32, 2, true);
      view.setUint16(34, 16, true);
      writeString(36, 'data');
      view.setUint32(40, length * 2, true);

      // Copy PCM data
      const offset = 44;
      for (let i = 0; i < length; i += 2) {
        const sample = (audioData[i + 1] << 8) | audioData[i];
        view.setInt16(offset + i, sample, true);
      }

      return buffer;
    }

    // Transcribe raw audio using OpenAI Whisper
    async function transcribeRawAudio() {
      if (!lastRawBase64) {
        log("‚ùå No raw audio to transcribe");
        return;
      }

      const transcribeBtn = document.getElementById("transcribeBtn");
      transcribeBtn.disabled = true;
      transcribeBtn.textContent = "üìù Transcribing...";

      try {
        // Convert base64 to blob for transcription API
        const binaryString = atob(lastRawBase64);
        const bytes = new Uint8Array(binaryString.length);
        
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        
        const wavData = createWavFile(bytes, 24000, 1);
        const blob = new Blob([wavData], { type: 'audio/wav' });
        
        log("üîÑ Sending to transcription API...");
        
        const formData = new FormData();
        formData.append('file', blob, 'raw_audio.wav');
        formData.append('model', 'whisper-1');
        formData.append('response_format', 'json');
        
        const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${API_KEY}`
          },
          body: formData
        });

        if (!response.ok) {
          const errorData = await response.json();
          throw new Error(`API error: ${response.status} - ${errorData.error?.message || 'Unknown error'}`);
        }

        const result = await response.json();
        const transcription = result.text;
        
        log(`‚úÖ Raw PCM Transcription: "${transcription}"`);
        displayTranscript("üéØ Raw PCM16 Transcribed", transcription, false, true);
        
      } catch (error) {
        log(`‚ùå Transcription error: ${error.message}`);
      } finally {
        transcribeBtn.disabled = false;
        transcribeBtn.textContent = "üìù Transcribe Raw Audio";
      }
    }

    // Display transcript
    function displayTranscript(speaker, text, isAI = false, isTranscription = false) {
      const container = document.getElementById("transcriptContainer");
      const transcriptDiv = document.createElement('div');
      
      if (isTranscription) {
        transcriptDiv.className = 'transcription-result';
      } else {
        transcriptDiv.className = isAI ? 'ai-text' : 'transcript';
      }
      
      transcriptDiv.innerHTML = `<strong>${speaker}:</strong> "${text}"`;
      container.appendChild(transcriptDiv);
      
      // Keep only last 10 transcripts
      while (container.children.length > 10) {
        container.removeChild(container.firstChild);
      }
    }

    // Connect to Realtime API
    async function connect() {
      const connectBtn = document.getElementById("connectBtn");
      const recordBtn = document.getElementById("recordBtn");
      
      connectBtn.disabled = true;
      connectBtn.textContent = "Connecting...";
      responseAudioChunks = [];

      try {
        ws = new WebSocket(
          "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview",
          [
            "realtime",
            "openai-insecure-api-key." + API_KEY,
            "openai-beta.realtime-v1"
          ]
        );

        ws.onopen = () => {
          log(" Connected to OpenAI Realtime API");
          isConnected = true;
          connectBtn.textContent = "Connected ‚úÖ";
          recordBtn.disabled = false;
          
          ws.send(JSON.stringify({
            type: "session.update",
            session: {
              type: "realtime",
              modalities: ["text", "audio"],
              instructions: "You are a helpful assistant. Respond naturally to whatever the user says.",
              voice: "alloy",
              output_audio_format: "pcm16",
              turn_detection: {
                type: "server_vad",
                threshold: 0.5,
                prefix_padding_ms: 300,
                silence_duration_ms: 500
              }
            }
          }));
        };

        ws.onmessage = (event) => {
          const data = JSON.parse(event.data);
          log(`üì® ${data.type}`);

          if (data.type === "conversation.item.input_audio_transcription.completed") {
            log(`üåê Realtime API Transcript: "${data.transcript}"`);
            displayTranscript("üåê Realtime API", data.transcript);
          }

          if (data.type === "response.text.delta") {
            aiTextBuffer += data.delta;
          }

          if (data.type === "response.text.done") {
            if (aiTextBuffer) {
              displayTranscript("ü§ñ AI responds", aiTextBuffer, true);
              aiTextBuffer = '';
            }
          }

          if (data.type === "response.audio.delta") {
            const binaryString = atob(data.delta);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
              bytes[i] = binaryString.charCodeAt(i);
            }
            responseAudioChunks.push(bytes);
          }

          if (data.type === "response.audio.done") {
            log("üéµ Creating AI response audio...");
            createResponseAudio();
          }

          if (data.type === "error") {
            log(`‚ùå API Error: ${JSON.stringify(data.error)}`);
          }
        };

        ws.onerror = (err) => {
          log(`‚ùå WebSocket Error: ${err.message || 'Unknown error'}`);
          resetConnection();
        };

        ws.onclose = (event) => {
          log(`üîí Connection closed (code: ${event.code})`);
          resetConnection();
        };

      } catch (error) {
        log(`‚ùå Connection failed: ${error.message}`);
        resetConnection();
      }
    }

    function resetConnection() {
      isConnected = false;
      document.getElementById("connectBtn").disabled = false;
      document.getElementById("connectBtn").textContent = "Connect to Realtime API";
      document.getElementById("recordBtn").disabled = true;
      document.getElementById("sendTextBtn").disabled = true;
      document.getElementById("uploadAudioBtn").disabled = true;
    }

    // Create playable audio from AI response
    function createResponseAudio() {
      if (responseAudioChunks.length === 0) return;

      const totalLength = responseAudioChunks.reduce((sum, chunk) => sum + chunk.length, 0);
      const combinedAudio = new Uint8Array(totalLength);
      let offset = 0;
      
      for (const chunk of responseAudioChunks) {
        combinedAudio.set(chunk, offset);
        offset += chunk.length;
      }

      const wavData = createWavFile(combinedAudio, 24000, 1);
      const blob = new Blob([wavData], { type: 'audio/wav' });
      const audioUrl = URL.createObjectURL(blob);

      const audioEl = document.getElementById("player");
      audioEl.src = audioUrl;
      audioEl.load();
      
      log("üé∂ AI response audio ready!");
      audioEl.play().catch(e => {
        log(`‚ö†Ô∏è Auto-play failed: ${e.message}`);
      });
    }

    // Event listeners
    document.getElementById("connectBtn").onclick = connect;
    document.getElementById("recordBtn").onclick = toggleRecording;
    document.getElementById("transcribeBtn").onclick = () => {
      if (lastRecordedBlob) {
        transcribeAudio(lastRecordedBlob);
      }
    };
  </script>
</body>
</html>
